"""
Basic statistical analysis functions.
"""

import json
import calendar
import logging
import pandas as pd
import numpy as np
from pandas import Timestamp

from src.core.config import USER_MAPPING

logger = logging.getLogger(__name__)


def merge_users(df):
    """Merge user names according to mapping."""
    df['username'] = df['username'].replace(USER_MAPPING)
    return df


def parse_date_str(x):
    """
    Parse date string with timezone awareness.
    First tries explicit format, then infers UTC if needed.
    """
    dt = pd.to_datetime(x, format="%Y-%m-%dT%H:%M:%S%z", errors='coerce')
    if pd.isna(dt):
        dt = pd.to_datetime(x, infer_datetime_format=True, utc=True, errors='coerce')
    return dt


def load_and_prepare_data(json_file, start_date=None, end_date=None):
    """
    Load message database from JSON file and prepare DataFrame.
    Selects only messages, parses dates, applies user mapping,
    and filters by optional date range.
    """
    with open(json_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    messages = data.get("messages", [])
    df = pd.DataFrame(messages)
    df_messages = df[df.get('type') == 'message'].copy()
    if df_messages.empty:
        return df_messages
    df_messages['date'] = df_messages['date'].apply(parse_date_str)
    df_messages['username'] = df_messages['from'].fillna('Unknown')
    df_messages = df_messages[df_messages['username'] != 'Unknown']
    df_messages = merge_users(df_messages)
    if start_date is not None:
        df_messages = df_messages[df_messages['date'] >= pd.to_datetime(start_date, utc=True)]
    if end_date is not None:
        if not isinstance(end_date, Timestamp):
            end_dt = pd.to_datetime(end_date, utc=True)
        else:
            end_dt = end_date if end_date.tzinfo is not None else end_date.tz_localize('UTC')
        df_messages = df_messages[df_messages['date'] <= end_dt]
    return df_messages


def get_global_top_users(df, top_n=7):
    """Determine global top-N users by message count."""
    user_counts = df['username'].value_counts().reset_index()
    user_counts.columns = ['username', 'message_count']
    top_users = user_counts.head(top_n)['username'].tolist()
    return top_users, user_counts


def prepare_relative_data(df, top_users, freq='W'):
    """Prepare data for relative stacked-area chart."""
    df_copy = df.copy()
    # Ensure date column is datetime
    if not pd.api.types.is_datetime64_any_dtype(df_copy['date']):
        df_copy['date'] = pd.to_datetime(df_copy['date'], utc=True)
    df_copy['interval'] = df_copy['date'].dt.to_period(freq).apply(lambda r: r.start_time)
    show_other = len(top_users) < df_copy['username'].nunique()
    if show_other:
        df_copy['category'] = df_copy['username'].apply(lambda x: x if x in top_users else '–î—Ä—É–≥–∏–µ')
    else:
        df_copy['category'] = df_copy['username']
    grouped = df_copy.groupby(['interval', 'category']).size().reset_index(name='count')
    totals = grouped.groupby('interval')['count'].sum().reset_index(name='total_count')
    merged = pd.merge(grouped, totals, on='interval')
    merged['relative_share'] = merged['count'] / merged['total_count']
    pivot = merged.pivot(index='interval', columns='category', values='relative_share').fillna(0)
    for u in top_users + (['–î—Ä—É–≥–∏–µ'] if show_other else []):
        if u not in pivot.columns:
            pivot[u] = 0
    pivot = pivot[top_users + (['–î—Ä—É–≥–∏–µ'] if show_other else [])]
    pivot['sum'] = pivot.sum(axis=1)
    if not (pivot['sum'].round(4) == 1).all():
        logger.warning("Sum of shares in some intervals is not 1.")
    return pivot.drop(columns=['sum'])


def prepare_cumulative_data(df, top_users, total_messages, freq='W'):
    """Prepare data for cumulative stacked-area chart."""
    df_copy = df.copy()
    # Ensure date column is datetime
    if not pd.api.types.is_datetime64_any_dtype(df_copy['date']):
        df_copy['date'] = pd.to_datetime(df_copy['date'], utc=True)
    df_copy['interval'] = df_copy['date'].dt.to_period(freq).apply(lambda r: r.start_time)
    show_other = len(top_users) < df_copy['username'].nunique()
    if show_other:
        df_copy['category'] = df_copy['username'].apply(lambda x: x if x in top_users else '–î—Ä—É–≥–∏–µ')
    else:
        df_copy['category'] = df_copy['username']
    grouped = df_copy.groupby(['interval', 'category']).size().reset_index(name='count')
    grouped = grouped.sort_values('interval')
    pivot = grouped.pivot(index='interval', columns='category', values='count').fillna(0)
    pivot = pivot[top_users + (['–î—Ä—É–≥–∏–µ'] if show_other else [])]
    pivot = pivot.cumsum() / total_messages
    pivot['sum'] = pivot.sum(axis=1)
    if not (pivot['sum'] <= 1).all():
        logger.warning("Sum of shares in some intervals exceeds 1.")
    return pivot.drop(columns=['sum'])


def build_monthly_stats_text(df, month_str):
    """Generate text summary of statistics for specified month."""
    total_msgs = len(df)
    if total_msgs == 0:
        return f"No messages found for {month_str}."
    user_counts = df['username'].value_counts().reset_index()
    user_counts.columns = ['username', 'message_count']
    user_counts['percentage'] = user_counts['message_count'] / total_msgs * 100
    top_user = user_counts.iloc[0]
    df['date_only'] = df['date'].dt.date
    day_counts = df.groupby('date_only').size()
    active_days = day_counts.count()
    avg_msgs = total_msgs / active_days if active_days else 0
    median_msgs = day_counts.median()
    std_msgs = day_counts.std()
    busiest_day = day_counts.idxmax() if not day_counts.empty else "N/A"
    busiest_day_count = day_counts.max() if not day_counts.empty else 0
    least_day = day_counts.idxmin() if not day_counts.empty else "N/A"
    least_day_count = day_counts.min() if not day_counts.empty else 0
    year, month = map(int, month_str.split('-'))
    total_days = calendar.monthrange(year, month)[1]
    active_pct = active_days / total_days * 100 if total_days else 0
    df['hour'] = df['date'].dt.hour
    hour_counts = df['hour'].value_counts().sort_index()
    busiest_hour = hour_counts.idxmax() if not hour_counts.empty else "N/A"
    busiest_hour_count = hour_counts.max() if not hour_counts.empty else 0
    text = f"Statistics for {month_str}:\n" + f"Total messages: {total_msgs}\n\n" + "Messages by user:\n"
    for i, row in user_counts.iterrows():
        if i >= 5: break
        text += f"{i + 1}. {row['username']} ‚Äî {row['message_count']} messages ({row['percentage']:.2f}%)\n"
    text += (
        f"\nMost active user: {top_user['username']} ({top_user['message_count']} messages)\n"
        f"\nDays in month: {total_days}, active days: {active_days} ({active_pct:.2f}%)\n"
        f"Average messages per active day: {avg_msgs:.2f}\n"
        f"Median messages per active day: {median_msgs:.2f}\n"
        f"Standard deviation: {std_msgs:.2f}\n"
        f"Busiest day: {busiest_day} ({busiest_day_count} messages)\n"
        f"Least active day: {least_day} ({least_day_count} messages)\n"
        f"\nBusiest hour: {busiest_hour:02d}:00 ({busiest_hour_count} messages)\n"
    )
    return text


def calculate_dominance_index(df):
    """
    Calculate dominance index showing how evenly messages are distributed among participants.
    Uses both Gini coefficient and Herfindahl-Hirschman Index (HHI).
    
    Args:
        df: DataFrame with messages containing 'username' column
        
    Returns:
        str: Formatted text with dominance analysis results
    """
    try:
        if df.empty:
            return "–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏–Ω–¥–µ–∫—Å–∞ –¥–æ–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏—è."
        
        # Calculate message counts per user
        user_counts = df['username'].value_counts().reset_index()
        user_counts.columns = ['username', 'message_count']
        
        total_messages = user_counts['message_count'].sum()
        user_counts['share'] = user_counts['message_count'] / total_messages
        
        # Calculate Gini coefficient
        def calculate_gini(shares):
            """Calculate Gini coefficient (0 = perfect equality, 1 = perfect inequality)"""
            # Sort shares in ascending order
            sorted_shares = np.sort(shares)
            n = len(sorted_shares)
            
            # Calculate cumulative shares
            cumulative_shares = np.cumsum(sorted_shares)
            
            # Calculate Gini using the formula
            gini = (n + 1 - 2 * np.sum((n + 1 - np.arange(1, n + 1)) * sorted_shares) / np.sum(sorted_shares)) / n
            
            return gini
        
        # Calculate Herfindahl-Hirschman Index (HHI)
        def calculate_hhi(shares):
            """Calculate HHI (0 = perfect competition, 10000 = monopoly)"""
            # HHI is sum of squared market shares (in percentage)
            hhi = np.sum((shares * 100) ** 2)
            return hhi
        
        # Calculate indices
        shares = user_counts['share'].values
        gini = calculate_gini(shares)
        hhi = calculate_hhi(shares)
        
        # Calculate normalized HHI for easier interpretation
        # Normalized HHI = (HHI - HHI_min) / (HHI_max - HHI_min)
        n_users = len(user_counts)
        hhi_min = 10000 / n_users  # Perfect equality
        hhi_max = 10000  # Monopoly
        if hhi_max > hhi_min:
            hhi_normalized = (hhi - hhi_min) / (hhi_max - hhi_min)
        else:
            hhi_normalized = 0
        
        # Determine dominance level based on indices
        if gini < 0.3:
            dominance_level = "üü¢ –ù–∏–∑–∫–∏–π"
            dominance_desc = "–°–æ–æ–±—â–µ–Ω–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω—ã –¥–æ–≤–æ–ª—å–Ω–æ —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ"
        elif gini < 0.5:
            dominance_level = "üü° –°—Ä–µ–¥–Ω–∏–π"
            dominance_desc = "–ï—Å—Ç—å –∑–∞–º–µ—Ç–Ω—ã–µ —Ä–∞–∑–ª–∏—á–∏—è –≤ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏"
        elif gini < 0.7:
            dominance_level = "üü† –í—ã—Å–æ–∫–∏–π"
            dominance_desc = "–ù–µ—Å–∫–æ–ª—å–∫–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –¥–æ–º–∏–Ω–∏—Ä—É—é—Ç –≤ —á–∞—Ç–µ"
        else:
            dominance_level = "üî¥ –û—á–µ–Ω—å –≤—ã—Å–æ–∫–∏–π"
            dominance_desc = "–ß–∞—Ç —Å–∏–ª—å–Ω–æ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∞–∫—Ç–∏–≤–Ω—ã—Ö —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤"
        
        # Calculate additional statistics
        top_5_share = user_counts.head(5)['share'].sum() * 100
        top_10_share = user_counts.head(10)['share'].sum() * 100
        
        # Find concentration points
        cumulative_share = 0
        users_for_50_percent = 0
        users_for_80_percent = 0
        
        for _, row in user_counts.iterrows():
            cumulative_share += row['share']
            if cumulative_share >= 0.5 and users_for_50_percent == 0:
                users_for_50_percent = _ + 1
            if cumulative_share >= 0.8 and users_for_80_percent == 0:
                users_for_80_percent = _ + 1
                break
        
        # Format results
        result = "üìä –ê–Ω–∞–ª–∏–∑ –∏–Ω–¥–µ–∫—Å–∞ –¥–æ–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏—è:\n\n"
        
        result += f"üéØ –£—Ä–æ–≤–µ–Ω—å –¥–æ–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏—è: {dominance_level}\n"
        result += f"{dominance_desc}\n\n"
        
        result += "üìà –ò–Ω–¥–µ–∫—Å—ã –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏–∏:\n"
        result += f"‚Ä¢ –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –î–∂–∏–Ω–∏: {gini:.3f}\n"
        result += f"  (0 = –∏–¥–µ–∞–ª—å–Ω–æ–µ —Ä–∞–≤–µ–Ω—Å—Ç–≤–æ, 1 = –º–æ–Ω–æ–ø–æ–ª–∏—è)\n"
        result += f"‚Ä¢ –ò–Ω–¥–µ–∫—Å –•–µ—Ä—Ñ–∏–Ω–¥–∞–ª—è-–•–∏—Ä—à–º–∞–Ω–∞: {hhi:.0f}\n"
        result += f"  (–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π: {hhi_normalized:.3f})\n\n"
        
        result += "üë• –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏:\n"
        result += f"‚Ä¢ –í—Å–µ–≥–æ —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤: {n_users}\n"
        result += f"‚Ä¢ –¢–æ–ø-5 –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: {top_5_share:.1f}% —Å–æ–æ–±—â–µ–Ω–∏–π\n"
        result += f"‚Ä¢ –¢–æ–ø-10 –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: {top_10_share:.1f}% —Å–æ–æ–±—â–µ–Ω–∏–π\n"
        result += f"‚Ä¢ 50% —Å–æ–æ–±—â–µ–Ω–∏–π –Ω–∞–ø–∏—Å–∞–ª–∏: {users_for_50_percent} —á–µ–ª–æ–≤–µ–∫(–∞)\n"
        result += f"‚Ä¢ 80% —Å–æ–æ–±—â–µ–Ω–∏–π –Ω–∞–ø–∏—Å–∞–ª–∏: {users_for_80_percent} —á–µ–ª–æ–≤–µ–∫(–∞)\n\n"
        
        # Top contributors
        result += "üèÜ –û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–Ω—Ç—Ä–∏–±—å—é—Ç–æ—Ä—ã:\n"
        for i, row in user_counts.head(5).iterrows():
            result += f"{i+1}. {row['username']} ‚Äî {row['message_count']} —Å–æ–æ–±—â–µ–Ω–∏–π ({row['share']*100:.1f}%)\n"
        
        # Interpretation
        result += "\nüí° –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è:\n"
        if gini < 0.3:
            result += "‚Ä¢ –ß–∞—Ç –∏–º–µ–µ—Ç –∑–¥–æ—Ä–æ–≤–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏\n"
            result += "‚Ä¢ –ë–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤ –≤–Ω–æ—Å—è—Ç —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –≤–∫–ª–∞–¥\n"
            result += "‚Ä¢ –ù–∏–∑–∫–∏–π —Ä–∏—Å–∫ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π"
        elif gini < 0.5:
            result += "‚Ä¢ –ê–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —É–º–µ—Ä–µ–Ω–Ω–æ —Å–∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∞\n"
            result += "‚Ä¢ –ï—Å—Ç—å –≥—Ä—É–ø–ø–∞ –æ—Å–Ω–æ–≤–Ω—ã—Ö —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤ –∏ –º–µ–Ω–µ–µ –∞–∫—Ç–∏–≤–Ω—ã–µ\n"
            result += "‚Ä¢ –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø–æ–æ—â—Ä—è—Ç—å —É—á–∞—Å—Ç–∏–µ –º–µ–Ω–µ–µ –∞–∫—Ç–∏–≤–Ω—ã—Ö"
        elif gini < 0.7:
            result += "‚Ä¢ –í—ã—Å–æ–∫–∞—è –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏\n"
            result += "‚Ä¢ –ß–∞—Ç –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –Ω–µ–±–æ–ª—å—à–æ–π –≥—Ä—É–ø–ø—ã –∞–∫—Ç–∏–≤–Ω—ã—Ö —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤\n"
            result += "‚Ä¢ –†–∏—Å–∫ —Å–Ω–∏–∂–µ–Ω–∏—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –ø—Ä–∏ —É—Ö–æ–¥–µ –∫–ª—é—á–µ–≤—ã—Ö —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤"
        else:
            result += "‚Ä¢ –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤—ã—Å–æ–∫–∞—è –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏—è\n"
            result += "‚Ä¢ –ß–∞—Ç —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏ –¥–µ—Ä–∂–∏—Ç—Å—è –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ª—é–¥—è—Ö\n"
            result += "‚Ä¢ –í—ã—Å–æ–∫–∏–π —Ä–∏—Å–∫ \"—Å–º–µ—Ä—Ç–∏\" —á–∞—Ç–∞ –ø—Ä–∏ –∏—Ö —É—Ö–æ–¥–µ"
        
        return result
        
    except Exception as e:
        logger.error(f"Error calculating dominance index: {e}")
        return f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞—Å—á–µ—Ç–µ –∏–Ω–¥–µ–∫—Å–∞ –¥–æ–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏—è: {str(e)}"


def analyze_user_correlation(df, period='D', min_messages=10):
    """
    Analyze correlation of activity between users.
    
    Args:
        df: DataFrame with messages containing 'date' and 'username' columns
        period: Time period for aggregation ('H', 'D', 'W')
        min_messages: Minimum messages for user to be included
        
    Returns:
        str: Formatted text with correlation analysis results
    """
    try:
        if df.empty:
            return "–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏."
        
        # Ensure date column is datetime
        df = df.copy()
        if not pd.api.types.is_datetime64_any_dtype(df['date']):
            df['date'] = pd.to_datetime(df['date'], utc=True)
        
        # Filter users with minimum messages
        user_counts = df['username'].value_counts()
        active_users = user_counts[user_counts >= min_messages].index.tolist()
        
        if len(active_users) < 2:
            return f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏. –ù–∞–π–¥–µ–Ω–æ {len(active_users)} –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π —Å –±–æ–ª–µ–µ —á–µ–º {min_messages} —Å–æ–æ–±—â–µ–Ω–∏—è–º–∏."
        
        df_active = df[df['username'].isin(active_users)]
        
        # Create time series for each user
        df_active['period'] = df_active['date'].dt.to_period(period)
        user_timeseries = df_active.groupby(['period', 'username']).size().unstack(fill_value=0)
        
        # Calculate correlation matrix
        correlation_matrix = user_timeseries.corr()
        
        # Find pairs with high correlation
        high_correlations = []
        for i in range(len(correlation_matrix)):
            for j in range(i + 1, len(correlation_matrix)):
                user1 = correlation_matrix.index[i]
                user2 = correlation_matrix.index[j]
                corr_value = correlation_matrix.iloc[i, j]
                
                if abs(corr_value) > 0.5:  # Threshold for significant correlation
                    high_correlations.append({
                        'user1': user1,
                        'user2': user2,
                        'correlation': corr_value
                    })
        
        # Sort by absolute correlation value
        high_correlations.sort(key=lambda x: abs(x['correlation']), reverse=True)
        
        # Format results
        result = "üìä –ê–Ω–∞–ª–∏–∑ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –º–µ–∂–¥—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º–∏:\n\n"
        
        period_names = {
            'H': '—á–∞—Å',
            'D': '–¥–µ–Ω—å',
            'W': '–Ω–µ–¥–µ–ª—è',
            'M': '–º–µ—Å—è—Ü'
        }
        period_name = period_names.get(period, period)
        
        result += f"üìÖ –ü–µ—Ä–∏–æ–¥ –∞–Ω–∞–ª–∏–∑–∞: {period_name}\n"
        result += f"üë• –ê–∫—Ç–∏–≤–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: {len(active_users)}\n"
        result += f"üìØ –ú–∏–Ω–∏–º—É–º —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: {min_messages}\n\n"
        
        if not high_correlations:
            result += "‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–∞—Ä –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π —Å –≤—ã—Å–æ–∫–æ–π –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–µ–π –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ (|–∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è| > 0.5)\n\n"
        else:
            result += "üîó –ü–∞—Ä—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π —Å –≤—ã—Å–æ–∫–æ–π –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–µ–π:\n\n"
            
            for i, pair in enumerate(high_correlations[:10], 1):  # Top 10 pairs
                corr = pair['correlation']
                if corr > 0:
                    emoji = "üìà"
                    desc = "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–∞—è"
                else:
                    emoji = "üìâ"
                    desc = "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–∞—è"
                
                result += f"{i}. {emoji} {pair['user1']} ‚ÜîÔ∏è {pair['user2']}\n"
                result += f"   –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è: {corr:.3f} ({desc})\n"
                
                # Interpret correlation
                if corr > 0.8:
                    result += "   üí° –û—á–µ–Ω—å —Å–∏–ª—å–Ω–∞—è —Å–≤—è–∑—å - –≤–æ–∑–º–æ–∂–Ω–æ, –ø–∏—à—É—Ç –≤ –æ–¥–Ω–æ –≤—Ä–µ–º—è\n"
                elif corr > 0.6:
                    result += "   üí° –°–∏–ª—å–Ω–∞—è —Å–≤—è–∑—å - —á–∞—Å—Ç–æ –∞–∫—Ç–∏–≤–Ω—ã –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ\n"
                elif corr > 0.4:
                    result += "   üí° –£–º–µ—Ä–µ–Ω–Ω–∞—è —Å–≤—è–∑—å\n"
                elif corr < -0.6:
                    result += "   üí° –°–∏–ª—å–Ω–∞—è –æ–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å - –∞–∫—Ç–∏–≤–Ω—ã –≤ —Ä–∞–∑–Ω–æ–µ –≤—Ä–µ–º—è\n"
                elif corr < -0.4:
                    result += "   üí° –£–º–µ—Ä–µ–Ω–Ω–∞—è –æ–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å\n"
                
                result += "\n"
        
        # Overall statistics
        all_correlations = []
        for i in range(len(correlation_matrix)):
            for j in range(i + 1, len(correlation_matrix)):
                all_correlations.append(correlation_matrix.iloc[i, j])
        
        if all_correlations:
            avg_corr = np.mean(all_correlations)
            median_corr = np.median(all_correlations)
            std_corr = np.std(all_correlations)
            
            result += "üìä –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π:\n"
            result += f"‚Ä¢ –°—Ä–µ–¥–Ω—è—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è: {avg_corr:.3f}\n"
            result += f"‚Ä¢ –ú–µ–¥–∏–∞–Ω–Ω–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è: {median_corr:.3f}\n"
            result += f"‚Ä¢ –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ: {std_corr:.3f}\n\n"
            
            # Interpretation
            result += "üí° –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è:\n"
            if avg_corr > 0.3:
                result += "‚Ä¢ –í —Ü–µ–ª–æ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ —Å–∫–ª–æ–Ω–Ω—ã –±—ã—Ç—å –∞–∫—Ç–∏–≤–Ω—ã–º–∏ –≤ –æ–¥–Ω–æ –≤—Ä–µ–º—è\n"
                result += "‚Ä¢ –í–æ–∑–º–æ–∂–Ω–æ, —ç—Ç–æ —Å–≤—è–∑–∞–Ω–æ —Å –æ–±—â–∏–º–∏ –∏–Ω—Ç–µ—Ä–µ—Å–∞–º–∏ –∏–ª–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –∑–æ–Ω–∞–º–∏\n"
            elif avg_corr < -0.1:
                result += "‚Ä¢ –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –∞–∫—Ç–∏–≤–Ω—ã –≤ —Ä–∞–∑–Ω–æ–µ –≤—Ä–µ–º—è\n"
                result += "‚Ä¢ –ß–∞—Ç –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è —Ä–∞–∑–Ω—ã–º–∏ –≥—Ä—É–ø–ø–∞–º–∏ –≤ —Ä–∞–∑–Ω—ã–µ –ø–µ—Ä–∏–æ–¥—ã\n"
            else:
                result += "‚Ä¢ –ê–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π —Å–ª–∞–±–æ —Å–≤—è–∑–∞–Ω–∞ –º–µ–∂–¥—É —Å–æ–±–æ–π\n"
                result += "‚Ä¢ –ö–∞–∂–¥—ã–π –∏–º–µ–µ—Ç —Å–≤–æ–π –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏\n"
        
        result += f"\nüìå –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, –Ω–∞—Å–∫–æ–ª—å–∫–æ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π.\n"
        result += "–ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–∞—è = –∞–∫—Ç–∏–≤–Ω—ã –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ, –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–∞—è = –≤ —Ä–∞–∑–Ω–æ–µ –≤—Ä–µ–º—è."
        
        return result
        
    except Exception as e:
        logger.error(f"Error analyzing user correlation: {e}")
        return f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏: {str(e)}"